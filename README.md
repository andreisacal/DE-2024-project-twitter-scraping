# Week 01 - Data Engineering Project for Scraping Tweets

üöÄ Introduction:

Welcome to my journey in the world of data engineering! As I embark on this exciting endeavour, I am thrilled to share with you my passion for mastering the skill of transforming raw data into valuable insights. With each passing weekend, I am committed to challenging myself to complete a new data engineering project in order to push the boundaries of my skills and knowledge.

Through this series of projects, I aim to not only deepen my understanding of fundamental data engineering concepts but also to improve my practical abilities in designing robust data pipelines, optimising data workflows and leveraging the power of tools and technologies.

As I embark on this journey into the world of data engineering, I've decided to start with something simple yet foundational. Over the coming weeks, I'll be delving into more complex pipelines, but for now, let me walk you through what I've accomplished.

üîç Project Overview:

For this initial project, I've focused on extracting data from Twitter using Python. Leveraging libraries and ScaperAPI, I've developed code that scrapes tweets and stores it as an CSV file. The real magic happens when this code is deployed onto an EC2 machine configured with Ubuntu. Here, I've set up Airflow to orchestrate the entire process. 

üõ†Ô∏è Technical Details:

Airflow, with its intuitive interface and robust scheduling capabilities, plays a central role in this project. By running the code developed in Python through Airflow, I automated the extraction process. The extracted data is then seamlessly stored in an AWS S3 folder, ready for further analysis or utilisation in downstream processes.

üìà Next Steps:

While this project represents a solid foundation, it's only the beginning. In the coming weeks, I'll be exploring more intricate pipelines, diving deeper into data manipulation, and leveraging the full potential of tools like Airflow and AWS.

üôè Thank You:

Thank you for joining me on this journey! I'm excited to share my progress and learnings as I go. Feel free to explore the code, provide feedback, or reach out with any questions or ideas.

Happy coding! üåü

## Project Architecture

![Untitled Diagram](https://github.com/andreisacal/W01-DE-Twtitter-Scraping/assets/166915179/3ae79e0f-50cb-464c-b246-51187d070f49)

## Getting Started

### Prerequisites
1. [AWS](https://aws.amazon.com/) account.
2. [ScraperAPI](https://www.scraperapi.com/) account.
3. IDE of your choice, I used [VSCode](https://code.visualstudio.com/)

### Local Testing

Opened VSCode, created a directory folder and inside the folder I created a file named "twitter_etl.py"
![image](https://github.com/andreisacal/W01-DE-Twtitter-Scraping/assets/166915179/045a3f37-8975-4241-a034-09fda2e10db1)
